{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "Substack Mention Tracker",
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substack Mention Tracker\n",
    "\n",
    "Track how often specific terms (e.g., **\"Claude Code\"**, **\"AI coding\"**) appear in Substack articles over time, grouped by month.\n",
    "\n",
    "This notebook uses Substack's undocumented search API to fetch article metadata, extract publication dates, and aggregate mention counts into a monthly timeline with a chart and downloadable CSV.\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| **API Endpoint** | `GET https://substack.com/api/v1/post/search` |\n",
    "| **Results per page** | 20 |\n",
    "| **Max pages** | ~100 (hard limit \u2248 2,000 results per query) |\n",
    "| **Authentication** | None required |\n",
    "\n",
    "> **Note:** This is an undocumented API discovered through reverse engineering. It may change without notice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Colab has `requests` and `matplotlib` pre-installed, but we run this cell to be safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Your Search\n",
    "\n",
    "Edit the variables below to customize which terms to track and how deep to search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration { display-mode: \"form\" }\n",
    "\n",
    "# \u2500\u2500 Edit these values \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "QUERIES = [\"Claude Code\", \"AI coding\"]  #@param {type:\"raw\"}\n",
    "MAX_PAGES_PER_QUERY = 100  #@param {type:\"integer\"}\n",
    "DELAY_BETWEEN_REQUESTS = 1.0  #@param {type:\"number\"}\n",
    "# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "print(f\"Queries:          {QUERIES}\")\n",
    "print(f\"Max pages/query:  {MAX_PAGES_PER_QUERY} ({MAX_PAGES_PER_QUERY * 20} max results)\")\n",
    "print(f\"Request delay:    {DELAY_BETWEEN_REQUESTS}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Core Functions\n",
    "\n",
    "These functions handle API requests, pagination, date grouping, and visualization. Run this cell \u2014 no edits needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "\n",
    "# \u2500\u2500 API Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "BASE_URL = \"https://substack.com/api/v1/post/search\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_search_page(query: str, page: int = 0) -> dict:\n",
    "    \"\"\"Fetch a single page of search results from Substack's API.\"\"\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"page\": page,\n",
    "        \"includePlatformResults\": \"true\",\n",
    "        \"filter\": \"all\",\n",
    "    }\n",
    "    resp = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def fetch_all_results(query: str, max_pages: int = 100, delay: float = 1.0) -> list:\n",
    "    \"\"\"Paginate through all search results for a query. Returns deduplicated post list.\"\"\"\n",
    "    all_results = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        try:\n",
    "            data = fetch_search_page(query, page)\n",
    "        except requests.exceptions.HTTPError as exc:\n",
    "            if exc.response is not None and exc.response.status_code in (400, 422):\n",
    "                break\n",
    "            raise\n",
    "\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        for post in results:\n",
    "            pid = post.get(\"id\")\n",
    "            if pid and pid not in seen_ids:\n",
    "                seen_ids.add(pid)\n",
    "                all_results.append(post)\n",
    "\n",
    "        has_more = data.get(\"more\", False)\n",
    "        print(\n",
    "            f\"  Page {page:>3d}: fetched {len(results):>2d} results \"\n",
    "            f\"(total unique: {len(all_results)})\"\n",
    "        )\n",
    "\n",
    "        if not has_more:\n",
    "            break\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def group_by_month(posts: list) -> dict:\n",
    "    \"\"\"Group posts by YYYY-MM and return sorted counts.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for post in posts:\n",
    "        date_str = post.get(\"post_date\", \"\")\n",
    "        if not date_str:\n",
    "            continue\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "            counts[dt.strftime(\"%Y-%m\")] += 1\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    return dict(sorted(counts.items()))\n",
    "\n",
    "\n",
    "def build_full_timeline(monthly_data: dict) -> list:\n",
    "    \"\"\"Build a continuous YYYY-MM timeline covering all queries.\"\"\"\n",
    "    all_months = set()\n",
    "    for counts in monthly_data.values():\n",
    "        all_months.update(counts.keys())\n",
    "    if not all_months:\n",
    "        return []\n",
    "\n",
    "    sorted_months = sorted(all_months)\n",
    "    start = datetime.strptime(sorted_months[0], \"%Y-%m\")\n",
    "    end = datetime.strptime(sorted_months[-1], \"%Y-%m\")\n",
    "\n",
    "    timeline = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        timeline.append(current.strftime(\"%Y-%m\"))\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def extract_pub_name(post: dict) -> str:\n",
    "    \"\"\"Safely extract the publication name from a post dict.\"\"\"\n",
    "    try:\n",
    "        bylines = post.get(\"publishedBylines\", [])\n",
    "        if bylines:\n",
    "            pub_users = bylines[0].get(\"publicationUsers\", [])\n",
    "            if pub_users:\n",
    "                return pub_users[0].get(\"publication\", {}).get(\"name\", \"Unknown\")\n",
    "    except (IndexError, KeyError, TypeError):\n",
    "        pass\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "print(\"Core functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fetch Data from Substack\n",
    "\n",
    "This cell queries the Substack search API for each term. Depending on `MAX_PAGES_PER_QUERY`, this may take a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Fetch all results \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "all_posts = {}    # query -> list of post dicts\n",
    "monthly_data = {} # query -> {month -> count}\n",
    "\n",
    "for query in QUERIES:\n",
    "    print(f'\\nFetching results for: \"{query}\"')\n",
    "    posts = fetch_all_results(query, max_pages=MAX_PAGES_PER_QUERY, delay=DELAY_BETWEEN_REQUESTS)\n",
    "    all_posts[query] = posts\n",
    "    monthly_data[query] = group_by_month(posts)\n",
    "    print(f\"  => {len(posts)} total posts found\")\n",
    "\n",
    "timeline = build_full_timeline(monthly_data)\n",
    "print(f\"\\nTimeline spans {len(timeline)} months: {timeline[0] if timeline else 'N/A'} to {timeline[-1] if timeline else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View Monthly Counts\n",
    "\n",
    "A pandas DataFrame showing the number of Substack articles mentioning each term per month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Build DataFrame \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "rows = []\n",
    "for month in timeline:\n",
    "    row = {\"Month\": month}\n",
    "    for query in QUERIES:\n",
    "        row[query] = monthly_data[query].get(month, 0)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add totals row\n",
    "totals = {\"Month\": \"TOTAL\"}\n",
    "for query in QUERIES:\n",
    "    totals[query] = df[query].sum()\n",
    "df_display = pd.concat([df, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "df_display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Trends\n",
    "\n",
    "A line chart showing how mentions have changed over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for query in QUERIES:\n",
    "    values = [monthly_data[query].get(m, 0) for m in timeline]\n",
    "    ax.plot(timeline, values, marker=\"o\", markersize=4, linewidth=2, label=query)\n",
    "\n",
    "ax.set_xlabel(\"Month\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of Substack Articles\", fontsize=12)\n",
    "ax.set_title(\"Monthly Mentions in Substack Articles\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "step = max(1, len(timeline) // 20)\n",
    "ax.set_xticks(range(0, len(timeline), step))\n",
    "ax.set_xticklabels(\n",
    "    [timeline[i] for i in range(0, len(timeline), step)],\n",
    "    rotation=45, ha=\"right\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "Run the cells below to download the data as CSV or detailed JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Download CSV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "csv_path = \"substack_mentions.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(csv_path)\n",
    "    print(f\"Downloading {csv_path}...\")\n",
    "except ImportError:\n",
    "    print(f\"CSV saved to: {csv_path}\")\n",
    "    print(\"(Run in Google Colab for automatic download)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Download detailed JSON (optional) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "export = {}\n",
    "for query, posts in all_posts.items():\n",
    "    export[query] = {\n",
    "        \"total_posts\": len(posts),\n",
    "        \"monthly_counts\": monthly_data[query],\n",
    "        \"posts\": [\n",
    "            {\n",
    "                \"id\": p.get(\"id\"),\n",
    "                \"title\": p.get(\"title\"),\n",
    "                \"post_date\": p.get(\"post_date\"),\n",
    "                \"canonical_url\": p.get(\"canonical_url\"),\n",
    "                \"publication_name\": extract_pub_name(p),\n",
    "                \"reaction_count\": p.get(\"reaction_count\", 0),\n",
    "                \"comment_count\": p.get(\"comment_count\", 0),\n",
    "                \"wordcount\": p.get(\"wordcount\", 0),\n",
    "            }\n",
    "            for p in sorted(posts, key=lambda x: x.get(\"post_date\", \"\"), reverse=True)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "json_path = \"substack_mentions_detailed.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(json_path)\n",
    "    print(f\"Downloading {json_path}...\")\n",
    "except ImportError:\n",
    "    print(f\"JSON saved to: {json_path}\")\n",
    "    print(\"(Run in Google Colab for automatic download)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Explore Top Articles (Bonus)\n",
    "\n",
    "View the most popular articles for each search term, ranked by reaction count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Top articles per query \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TOP_N = 10\n",
    "\n",
    "for query, posts in all_posts.items():\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'  Top {TOP_N} articles for \"{query}\"')\n",
    "    print(f'{\"=\" * 60}')\n",
    "\n",
    "    sorted_posts = sorted(posts, key=lambda p: p.get(\"reaction_count\", 0), reverse=True)\n",
    "\n",
    "    for i, p in enumerate(sorted_posts[:TOP_N], 1):\n",
    "        title = p.get(\"title\", \"Untitled\")\n",
    "        reactions = p.get(\"reaction_count\", 0)\n",
    "        date = p.get(\"post_date\", \"\")[:10]\n",
    "        pub = extract_pub_name(p)\n",
    "        url = p.get(\"canonical_url\", \"\")\n",
    "        print(f\"\\n  {i:>2}. [{reactions:>4} likes] {title}\")\n",
    "        print(f\"      {pub} \u00b7 {date}\")\n",
    "        print(f\"      {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes & Limitations\n",
    "\n",
    "1. **Undocumented API.** The endpoint `substack.com/api/v1/post/search` is not part of Substack's official API and could change at any time.\n",
    "\n",
    "2. **~2,000 result cap per query.** Substack returns at most ~100 pages of 20 results. For very popular terms, older results may be truncated. Consider running monthly and accumulating data.\n",
    "\n",
    "3. **Relevance-based search.** Results are ranked by relevance, not strict substring matching. A search for \"AI coding\" may return articles mentioning \"AI\" and \"coding\" separately.\n",
    "\n",
    "4. **Rate limiting.** A 1-second delay between requests is built in. Increase `DELAY_BETWEEN_REQUESTS` if you encounter errors.\n",
    "\n",
    "5. **Customization.** Edit the `QUERIES` list in Step 2 to track any terms you like. You can add as many as you want.\n"
   ]
  }
 ]
}